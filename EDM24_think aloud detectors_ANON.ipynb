{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e9e6caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joycesfolder/opt/anaconda3/lib/python3.8/site-packages/scipy/__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import openai\n",
    "\n",
    "from sklearn.metrics import classification_report, roc_curve, roc_auc_score, confusion_matrix, accuracy_score, f1_score, cohen_kappa_score\n",
    "from sklearn.model_selection import GroupKFold, train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn import tree, metrics\n",
    "import xgboost\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "\n",
    "random.seed(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e52037e",
   "metadata": {},
   "source": [
    "### import data and set up 5 folds cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59d9ddcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "df = pd.read_csv(\"think_aloud_all_platforms_2023Nov27.csv\") # this csv contains all valid input, in attempt level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43ab5265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data for CV\n",
    "\n",
    "group_dict = dict()\n",
    "groups = np.array([])\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    s_id = row['anon_student_id']\n",
    "    if s_id not in group_dict:\n",
    "        group_dict[s_id] = index\n",
    "    groups = np.append(groups, group_dict[s_id])\n",
    "    \n",
    "# Set up the splitter with 5 splits\n",
    "gkf = GroupKFold(n_splits = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5f4821",
   "metadata": {},
   "source": [
    "### USE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c63913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load universal sentence encoder\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-large/5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ee5acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Converting text into embedding - USE\n",
    "text = np.array(df['utterance_combined'], dtype=object)[:, np.newaxis]\n",
    "\n",
    "X = []\n",
    "for r in tqdm(text):\n",
    "    emb = embed(r)\n",
    "    review_emb = tf.reshape(emb, [-1]).numpy()\n",
    "    X.append(review_emb)\n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9b5883",
   "metadata": {},
   "source": [
    "### OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fed439b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load openAI key REMOVE when upload to github (TODO)\n",
    "openai.api_key = 'ADD YOUR KEY HERE' #api key\n",
    "\n",
    "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
    "   text = text.replace(\"\\n\", \" \")\n",
    "   return openai.Embedding.create(input = [text], model=model)['data'][0]['embedding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e59e4161",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 955/955 [03:12<00:00,  4.95it/s]\n"
     ]
    }
   ],
   "source": [
    "## Converting text into embedding - OpenAI\n",
    "text = df['utterance_combined']\n",
    "\n",
    "X = []\n",
    "for r in tqdm(text):\n",
    "    emb = get_embedding(r)\n",
    "    review_emb = tf.reshape(emb, [-1]).numpy()\n",
    "    X.append(review_emb)\n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e8a8f9",
   "metadata": {},
   "source": [
    "### prediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b188f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define label\n",
    "y = df.wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b71124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up storage arrays for each round of validation\n",
    "roc_auc_scores_all = np.array([])\n",
    "f1_scores_all = np.array([])\n",
    "kappa_all = np.array([])\n",
    "accuracy_all = np.array([])\n",
    "pred = pd.DataFrame()\n",
    "\n",
    "\n",
    "for train_index, test_index in gkf.split(X, y, groups=groups):\n",
    "    \n",
    "    # Get the training and test data from the dataset for this group\n",
    "    X_train = X[train_index]\n",
    "    X_test = X[test_index]\n",
    "    y_train = y.iloc[train_index]\n",
    "    y_test = y.iloc[test_index]\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(28, input_shape=(1536,), activation='relu')) \n",
    "    model.add(Dense(28, activation='relu')) \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(lr=0.01)\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer= optimizer,metrics = ['acc'])\n",
    "    \n",
    "    \n",
    "    num_epochs = 30\n",
    "    batch_size = 10\n",
    "\n",
    "    model.fit(\n",
    "        X_train, \n",
    "        y_train, \n",
    "        epochs=num_epochs, \n",
    "        validation_split=0.1,\n",
    "        shuffle=True, \n",
    "        batch_size=batch_size)\n",
    "    \n",
    "    # test classifier on this round of testing group\n",
    "    predictions = model.predict(X_test)\n",
    "    predictions_binary = (predictions >= 0.5).astype(int)\n",
    "    \n",
    "    pred_new = pd.concat([\n",
    "    pd.DataFrame(df.iloc[test_index,[0]]).reset_index(drop=True), #row number\n",
    "    pd.DataFrame(y.iloc[test_index]).reset_index(drop=True),\n",
    "    pd.DataFrame(predictions).reset_index(drop=True)],ignore_index=True, axis = 1)\n",
    "    \n",
    "    pred = pred.append(pred_new, ignore_index=True)\n",
    "    \n",
    "    # compute some metrics and store them for averaging later on\n",
    "   \n",
    "    # AUC\n",
    "    roc_auc_scores = roc_auc_score(y_test, predictions)\n",
    "    roc_auc_scores_all = np.append(roc_auc_scores_all, roc_auc_scores)\n",
    "    \n",
    "\n",
    "    \n",
    "# print mean scores for the 5-fold CV\n",
    "print(\"average roc_auc: \", np.round(roc_auc_scores_all.mean(), 3))\n",
    "print(\"stdv roc_auc: \", np.round(roc_auc_scores_all.std(), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b072551",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_use = pd.concat(\n",
    "    [pd.DataFrame(pred_use_process),\n",
    "     pd.DataFrame(pred_use_plan),\n",
    "     pd.DataFrame(pred_use_act),\n",
    "     pd.DataFrame(pred_use_wrong),\n",
    "    ], axis=1)\n",
    "\n",
    "pred_use.to_csv(\"pred_use.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75d531e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_openai_v3 = pd.concat(\n",
    "    [pd.DataFrame(pred_openai_process),\n",
    "     pd.DataFrame(pred_openai_plan),\n",
    "     pd.DataFrame(pred_openai_act),\n",
    "     pd.DataFrame(pred_openai_wrong),\n",
    "    ], axis=1)\n",
    "\n",
    "pred_openai_v3.to_csv(\"pred_openai_v3.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c27d0ea",
   "metadata": {},
   "source": [
    "## Transferability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6e6b6b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = df[df['platform'].isin(['ORCCA','Logic Tutor'])].index\n",
    "test_indices = df[df['platform'] == 'Stoich'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9e2d0f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "15/15 [==============================] - 1s 10ms/step - loss: 0.5974 - acc: 0.9172 - val_loss: 0.4479 - val_acc: 0.9412\n",
      "Epoch 2/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.3649 - acc: 0.9586 - val_loss: 0.2331 - val_acc: 0.9412\n",
      "Epoch 3/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.2144 - acc: 0.9586 - val_loss: 0.1566 - val_acc: 0.9412\n",
      "Epoch 4/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.1606 - acc: 0.9586 - val_loss: 0.1395 - val_acc: 0.9412\n",
      "Epoch 5/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.1418 - acc: 0.9586 - val_loss: 0.1319 - val_acc: 0.9412\n",
      "Epoch 6/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.1286 - acc: 0.9586 - val_loss: 0.1212 - val_acc: 0.9412\n",
      "Epoch 7/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.1134 - acc: 0.9586 - val_loss: 0.1167 - val_acc: 0.9412\n",
      "Epoch 8/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.1006 - acc: 0.9586 - val_loss: 0.1083 - val_acc: 0.9412\n",
      "Epoch 9/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0865 - acc: 0.9655 - val_loss: 0.0985 - val_acc: 0.9412\n",
      "Epoch 10/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0757 - acc: 0.9724 - val_loss: 0.0898 - val_acc: 0.9412\n",
      "Epoch 11/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0627 - acc: 0.9793 - val_loss: 0.0880 - val_acc: 0.9412\n",
      "Epoch 12/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0507 - acc: 0.9793 - val_loss: 0.0853 - val_acc: 0.9412\n",
      "Epoch 13/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0399 - acc: 0.9862 - val_loss: 0.0889 - val_acc: 0.9412\n",
      "Epoch 14/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0320 - acc: 0.9931 - val_loss: 0.0890 - val_acc: 0.9412\n",
      "Epoch 15/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0251 - acc: 1.0000 - val_loss: 0.0863 - val_acc: 0.9412\n",
      "Epoch 16/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0205 - acc: 1.0000 - val_loss: 0.0860 - val_acc: 0.9412\n",
      "Epoch 17/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0171 - acc: 1.0000 - val_loss: 0.0875 - val_acc: 0.9412\n",
      "Epoch 18/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0138 - acc: 1.0000 - val_loss: 0.0832 - val_acc: 0.9412\n",
      "Epoch 19/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0112 - acc: 1.0000 - val_loss: 0.0831 - val_acc: 0.9412\n",
      "Epoch 20/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0097 - acc: 1.0000 - val_loss: 0.0826 - val_acc: 0.9412\n",
      "Epoch 21/30\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 0.0080 - acc: 1.0000 - val_loss: 0.0832 - val_acc: 0.9412\n",
      "Epoch 22/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0068 - acc: 1.0000 - val_loss: 0.0842 - val_acc: 0.9412\n",
      "Epoch 23/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0058 - acc: 1.0000 - val_loss: 0.0831 - val_acc: 0.9412\n",
      "Epoch 24/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0052 - acc: 1.0000 - val_loss: 0.0834 - val_acc: 0.9412\n",
      "Epoch 25/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0045 - acc: 1.0000 - val_loss: 0.0856 - val_acc: 0.9412\n",
      "Epoch 26/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0040 - acc: 1.0000 - val_loss: 0.0834 - val_acc: 0.9412\n",
      "Epoch 27/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0036 - acc: 1.0000 - val_loss: 0.0853 - val_acc: 0.9412\n",
      "Epoch 28/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0031 - acc: 1.0000 - val_loss: 0.0860 - val_acc: 0.9412\n",
      "Epoch 29/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0028 - acc: 1.0000 - val_loss: 0.0852 - val_acc: 0.9412\n",
      "Epoch 30/30\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0026 - acc: 1.0000 - val_loss: 0.0856 - val_acc: 0.9412\n",
      "15/15 [==============================] - 0s 944us/step\n",
      "0.8539104024297647\n",
      "0.11794734013970987\n"
     ]
    }
   ],
   "source": [
    "y = df.wrong\n",
    "\n",
    "X_train = X[train_indices]\n",
    "X_test = X[test_indices]\n",
    "y_train = y.iloc[train_indices]\n",
    "y_test = y.iloc[test_indices]\n",
    "    \n",
    "model = Sequential()\n",
    "model.add(Dense(28, input_shape=(1536,), activation='relu')) \n",
    "model.add(Dense(28, activation='relu')) \n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.01)\n",
    "model.compile(loss = 'binary_crossentropy', optimizer= optimizer,metrics = ['acc'])\n",
    "    \n",
    "    \n",
    "num_epochs = 30\n",
    "batch_size = 10\n",
    "model.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    epochs=num_epochs,\n",
    "    validation_split=0.1,\n",
    "    shuffle=True, \n",
    "    batch_size=batch_size)\n",
    "\n",
    "# test classifier on this round of testing group\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "print(roc_auc_score(y_test, predictions))\n",
    "ci.auc(y_test, predictions) \n",
    "print(cohen_kappa_score(y_test, predictions.round()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ebb8a11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(predictions).to_csv(\"results/gpt_predStoich_wrong.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
